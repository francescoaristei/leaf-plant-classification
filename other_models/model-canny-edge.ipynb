{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport tensorflow as tf\nimport numpy as np\nimport os\nimport random\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom sklearn.utils import class_weight\nimport math \n\nfrom tensorflow.keras.applications import VGG16\nfrom tensorflow.keras.preprocessing import image\nfrom sklearn.metrics import confusion_matrix, classification_report\n\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\ntfk = tf.keras\ntfkl = tf.keras.layers\n\n!pip install split-folders\nimport splitfolders","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fix Random seed for reproducibility\nseed = 2113\n\nrandom.seed(seed)\nos.environ['PYTHONHASHSEED'] = str(seed)\nnp.random.seed(seed)\ntf.random.set_seed(seed)\ntf.compat.v1.set_random_seed(seed)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dataset folder\ndataset_dir = '../input/cannyleavesdataset' # dataset obtained by applying the canny edge to the original dataset\n                                            # via python script (see at the end of the notebook, after post processing)\ntrain_dir = os.path.join(dataset_dir, 'training')\n# Labels\nlabels = [\n    \"Apple\",\n    \"Blueberry\",\n    \"Cherry\",\n    \"Corn\",\n    \"Grape\",\n    \"Orange\",\n    \"Peach\",\n    \"Pepper\",\n    \"Potato\",\n    \"Raspberry\",\n    \"Soybean\",\n    \"Squash\",\n    \"Strawberry\",\n    \"Tomato\"\n]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot example images from dataset\nnum_row = len(labels)//2\nnum_col = len(labels)//num_row\nfig, axes = plt.subplots(num_row, num_col, figsize=(2*num_row,15*num_col))\nfor i in range(len(labels)):\n    if i < len(labels):\n        class_imgs = next(os.walk('{}/{}/'.format(train_dir, labels[i])))[2]\n        class_img = class_imgs[0]\n        img = Image.open('{}/{}/{}'.format(train_dir, labels[i], class_img))\n        ax = axes[i//num_col, i%num_col]\n        ax.imshow(np.array(img))\n        ax.set_title('{}'.format(labels[i]))\nplt.tight_layout()\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Dataset Preprocessing","metadata":{}},{"cell_type":"code","source":"# Apply splitfolders to split the dataset in train/validation/test sets\nsplitfolders.ratio(train_dir, output=\"output\", seed=seed, ratio=(.7, .2, .1), group_prefix=None)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dataset folders \noutput_dir = './output'\ntraining_dir = os.path.join(output_dir, 'train')\nvalidation_dir = os.path.join(output_dir, 'val')\ntest_dir = os.path.join(output_dir, 'test')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create ImageDataGenerator object\n# Apply DataAugmentation to the train set\ntrain_data_gen = ImageDataGenerator(rotation_range=45,\n                          zoom_range=0.4,\n                          horizontal_flip=True,\n                          vertical_flip=True,\n                          width_shift_range=0.2,\n                          height_shift_range=0.2,\n                          rescale=1/255)\n# Do not apply Data Augmentation on validation and test set\nvalid_data_gen = ImageDataGenerator(rescale=1/255)\ntest_data_gen = ImageDataGenerator(rescale=1/255)\n\ntrain_gen = train_data_gen.flow_from_directory(directory=training_dir, \n                                              target_size=(256, 256), #when i read the images, this will resize as you specify\n                                              color_mode='rgb',\n                                              classes=None, #default is None (read the classes into a lexicographic order and assign to them the indexes)\n                                              batch_size=20,\n                                              shuffle=True, #each epoch shuffle the data. \n                                              seed=seed) \nvalid_gen = valid_data_gen.flow_from_directory(directory=validation_dir, \n                                              target_size=(256, 256),\n                                              color_mode='rgb',\n                                              classes=None,\n                                              batch_size=20,\n                                              shuffle=False,\n                                              seed=seed)\n\ntest_gen = test_data_gen.flow_from_directory(directory=test_dir, \n                                              target_size=(256, 256),\n                                              color_mode='rgb',\n                                              classes=None,\n                                              batch_size=20,\n                                              shuffle=False,\n                                              seed=seed)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compute weights for each class\nclass_weights = class_weight.compute_class_weight(\n           'balanced',\n            classes=np.unique(train_gen.classes), \n            y=train_gen.classes)\n\ndataset_stats = {}\n\nfor i, label in enumerate(labels):\n    files = os.listdir(os.path.join(train_dir, label))\n    dataset_stats[i] = [label, len(files)]\ndf = pd.DataFrame.from_dict(dataset_stats, orient=\"index\", columns=[\"Category\", \"Size\"])\ndf[\"Weight\"] = class_weights\n\ndf","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"weights={}\n\nfor i in range(len(class_weights)):\n    weights[i] = class_weights[i]\n\nweights","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_shape = (256, 256, 3)\nepochs = 200","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model","metadata":{}},{"cell_type":"code","source":"from datetime import datetime\n\ndef create_folders_and_callbacks(model_name):\n    # List of callbacks\n    callbacks = []\n    \n    # Create data_loader folder if it does not exist\n    exps_dir = os.path.join('data_loaded')\n    if not os.path.exists(exps_dir):\n        os.makedirs(exps_dir)\n\n    now = datetime.now().strftime('%b%d_%H-%M-%S')\n\n    exp_dir = os.path.join(exps_dir, model_name + '_' + str(now))\n    if not os.path.exists(exp_dir):\n        os.makedirs(exp_dir)\n\n  # Model checkpoint: allows to automatically save the model during training\n\n    ckpt_dir = os.path.join(exp_dir, 'ckpts')  # create a subfolder in the folder which will contain the model checkpoints\n    if not os.path.exists(ckpt_dir):\n        os.makedirs(ckpt_dir)\n\n    ckpt_callback = \\\n        tf.keras.callbacks.ModelCheckpoint(\n            filepath=os.path.join(ckpt_dir, 'cp'),\n            save_weights_only=False,\n            save_best_only=False)  # where i want to save the checkpoints\n\n    callbacks.append(ckpt_callback)\n\n  # Visualize Learning on Tensorboard\n  # ---------------------------------\n\n    tb_dir = os.path.join(exp_dir, 'tb_logs')\n    if not os.path.exists(tb_dir):\n        os.makedirs(tb_dir)\n\n    tb_callback = tf.keras.callbacks.TensorBoard(\n        log_dir=tb_dir,\n        profile_batch=0,\n        histogram_freq=1)  # allows to save the histogram of weights over the epochs (the value stands for the epoch frequency)\n    \n    callbacks.append(tb_callback)\n\n  # Early Stopping\n  # --------------\n\n    es_callback = tf.keras.callbacks.EarlyStopping(\n        monitor=\"val_loss\",\n        patience=8,\n        min_delta=0.0003,\n        restore_best_weights=True)\n    \n    callbacks.append(es_callback)\n    \n    return callbacks","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = tfk.Sequential([\n    layers.Input(shape=input_shape),\n    \n    layers.SeparableConv2D(32, 3, activation='relu'), # Less parameters than Conv2D with same performances\n    layers.SeparableConv2D(64, 3, activation='relu'),\n    layers.BatchNormalization(), # used to stabilize and speed up the training\n    layers.MaxPooling2D(2),\n    \n    layers.SeparableConv2D(64, 3, activation='relu'),\n    layers.SeparableConv2D(128, 3, activation='relu'),\n    layers.BatchNormalization(),\n    layers.MaxPooling2D(2),\n    \n    layers.SeparableConv2D(128, 3, activation='relu'),\n    layers.SeparableConv2D(256, 3, activation='relu'),\n    layers.BatchNormalization(),\n    layers.GlobalAveragePooling2D(), # we use GlobalAveragePooling instead of Flatten, since it reduces the number of parameters\n    \n    layers.Flatten(),\n    layers.BatchNormalization(renorm=True),\n    layers.Dropout(0.5, seed=seed), # add dropout to reduce overfitting\n    layers.Dense(256, activation='relu'),\n    layers.Dense(len(labels), activation='softmax')\n])\n\nmodel.compile(\n    optimizer=tfk.optimizers.RMSprop(lr=1e-4),\n    loss=tfk.losses.CategoricalCrossentropy(),\n    metrics=['accuracy'],\n)\n\nmodel.summary()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create folders and callbacks and fit\ncallbacks = create_folders_and_callbacks(model_name='CNN_Canny')\n\n# Train the model\nhistory = model.fit(\n    x = train_gen,\n    validation_data=valid_gen,\n    epochs=epochs,\n    batch_size=32,\n    class_weight=weights,\n    callbacks=callbacks\n)\n\nmodel.save(\"CNN_Canny\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Postprocessing","metadata":{}},{"cell_type":"code","source":"# Predict the test set with the CNN\npredictions = model.predict(test_gen)\npredictions.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Confution Matrix and Classification Report\nY_pred = xe.predict_generator(test_gen, 1786 // 20+1)\ny_pred = np.argmax(Y_pred, axis=1)\nprint('Confusion Matrix')\nplt.figure(figsize=(10,8))\ncm = confusion_matrix(test_gen.classes, y_pred)\nsns.heatmap(cm.T, xticklabels=labels, yticklabels=labels)\nplt.xlabel('True labels')\nplt.ylabel('Predicted labels')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Classification Report')\nprint(classification_report(test_gen.classes, y_pred, target_names=labels))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the target images and the predictions\nbatch = next(test_gen)\nbatch_pr = model.predict(batch[0])\nind=random.randint(0,19)\nfig, (ax1, ax2) = plt.subplots(1,2)\nfig.set_size_inches(18,5)\n\nimage = batch[0][ind]\nax1.imshow(image)\ncat_label = batch[1][ind]\n\n\nlabel_index = np.argmax(cat_label)\nax1.set_title('True label: '+labels[label_index])\nax2.barh(labels, batch_pr[ind], color=plt.get_cmap('Paired').colors, log=True)\nax2.set_title('Predicted label: '+labels[np.argmax(batch_pr[ind])])\nax2.grid(alpha=.3)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"layers = [layer.output for layer in model.layers[2].layers if isinstance(layer, tf.keras.layers.Conv2D)]\nactivation_model = tf.keras.Model(inputs=model.layers[2].input, outputs=layers)\nfmaps = activation_model.predict(tf.expand_dims(image, 0))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom mpl_toolkits.axes_grid1 import ImageGrid\n%matplotlib inline\ndef display_activation(fmaps, depth=0, first_n=-1):\n\n    fmaps = fmaps[depth] \n    if first_n > 0:\n        fmaps = fmaps[0, :, :, :first_n] \n        fmaps = tf.image.resize(fmaps, size=[128, 128]) \n\n    # Distribute on a grid for plotting\n    col_size = 8\n    row_size = fmaps.shape[-1] // 8\n    fmap_channel=0\n    fig = plt.figure(figsize=(30, 30))\n    grid = ImageGrid(fig, 111,  \n                    nrows_ncols=(row_size, col_size),  \n                    axes_pad=0.1,  \n                    )\n    for row in range(0,row_size):\n        for col in range(0,col_size):\n            grid[fmap_channel].imshow(fmaps[0, :, :, fmap_channel], cmap='gray', aspect='auto')\n            fmap_channel += 1\n    plt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display_activation(fmaps=fmaps, depth=1, first_n=-1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Canny Edge Detector Algorithm","metadata":{}},{"cell_type":"code","source":"# Script used to preprocess the images with Canny Edge Algorithm and obtain a new dataset\n\nfrom scipy import ndimage\nfrom scipy.ndimage.filters import convolve\n\nfrom scipy import misc\nimport numpy as np\n    \n# reduction of the noise in the image via Guassian kernel\ndef gaussian_kernel(size, sigma=1): # size = kernel size, sigma = gaussian noise\n    size = int(size) // 2\n    x, y = np.mgrid[-size:size+1, -size:size+1]\n    normal = 1 / (2.0 * np.pi * sigma**2)\n    g =  np.exp(-((x**2 + y**2) / (2.0*sigma**2))) * normal\n    return g\n\n# Application of sobel filter to extract the edges\ndef sobel_filters(img):\n    Kx = np.array([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], np.float32)\n    Ky = np.array([[1, 2, 1], [0, 0, 0], [-1, -2, -1]], np.float32)\n\n    Ix = ndimage.filters.convolve(img, Kx)\n    Iy = ndimage.filters.convolve(img, Ky)\n\n    G = np.hypot(Ix, Iy)\n    G = G / G.max() * 255\n    theta = np.arctan2(Iy, Ix)\n    return (G, theta)\n\n# To thin out the edges of the image after sobel preprocessing\ndef non_max_suppression(img, D):\n    M, N = img.shape\n    Z = np.zeros((M,N), dtype=np.int32)\n    angle = D * 180. / np.pi\n    angle[angle < 0] += 180\n\n\n    for i in range(1,M-1):\n        for j in range(1,N-1):\n            try:\n                q = 255\n                r = 255\n\n               #angle 0\n                if (0 <= angle[i,j] < 22.5) or (157.5 <= angle[i,j] <= 180):\n                    q = img[i, j+1]\n                    r = img[i, j-1]\n                #angle 45\n                elif (22.5 <= angle[i,j] < 67.5):\n                    q = img[i+1, j-1]\n                    r = img[i-1, j+1]\n                #angle 90\n                elif (67.5 <= angle[i,j] < 112.5):\n                    q = img[i+1, j]\n                    r = img[i-1, j]\n                #angle 135\n                elif (112.5 <= angle[i,j] < 157.5):\n                    q = img[i-1, j-1]\n                    r = img[i+1, j+1]\n\n                if (img[i,j] >= q) and (img[i,j] >= r):\n                      Z[i,j] = img[i,j]\n                else:\n                    Z[i,j] = 0\n\n\n            except IndexError as e:\n                pass\n\n    return Z\n\n# To identify the weak (pixels whose intensity value is not enough to be considered as strong ones  \n# but yet not small enough to be considered as non-relevant for the edge detection)\n# strong (pixels that have an intensity so high that we are sure they contribute to the final edge) and non-relevant pixels\ndef threshold(img):\n\n    highThreshold = img.max() * 0.15;\n    lowThreshold = highThreshold * 0.05;\n\n    M, N = img.shape\n    res = np.zeros((M,N), dtype=np.int32)\n\n    weak = np.int32(75)\n    strong = np.int32(255)\n\n    strong_i, strong_j = np.where(img >= highThreshold)\n    zeros_i, zeros_j = np.where(img < lowThreshold)\n\n    weak_i, weak_j = np.where((img <= highThreshold) & (img >= lowThreshold))\n\n    res[strong_i, strong_j] = strong\n    res[weak_i, weak_j] = weak\n\n    return (res)\n\n# transforming weak pixels into strong ones, if and only if at least one \n# of the pixels around the one being processed is a strong one,\ndef hysteresis(img):\n\n    M, N = img.shape\n    weak = 75\n    strong = 255\n\n    for i in range(1, M-1):\n        for j in range(1, N-1):\n            if (img[i,j] == weak):\n                try:\n                    if ((img[i+1, j-1] == strong) or (img[i+1, j] == strong) or (img[i+1, j+1] == strong)\n                        or (img[i, j-1] == strong) or (img[i, j+1] == strong)\n                        or (img[i-1, j-1] == strong) or (img[i-1, j] == strong) or (img[i-1, j+1] == strong)):\n                        img[i, j] = strong\n                    else:\n                        img[i, j] = 0\n                except IndexError as e:\n                    pass\n\n    return img\n\ndef canny_edge(image):\n    image = np.dot(image[...,:3], [0.2989, 0.5870, 0.1140]) # rgb to grayscale conversion\n    img_smoothed = convolve(image, gaussian_kernel(5, 1))\n    gradientMat, thetaMat = sobel_filters(img_smoothed)\n    nonMaxImg = non_max_suppression(gradientMat, thetaMat)\n    thresholdImg = threshold(nonMaxImg)\n    img_final = hysteresis(thresholdImg)\n    img_final = np.stack((img_final,)*3, axis=-1) # grayscale to rgb conversion\n    img_final = np.array(img_final)\n\n    return img_final","metadata":{},"execution_count":null,"outputs":[]}]}